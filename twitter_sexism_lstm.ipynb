{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, BatchNormalization, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#local module that I created to help with text preprocessing\n",
    "import text_processing as t_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              label\n",
      "count  43177.000000\n",
      "mean       0.120226\n",
      "std        0.325229\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        0.000000\n",
      "max        1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15825</th>\n",
       "      <td>0</td>\n",
       "      <td>... such evil acts!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7656</th>\n",
       "      <td>0</td>\n",
       "      <td>ditching @user for prom really was #sad   than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28479</th>\n",
       "      <td>0</td>\n",
       "      <td>ready for takeoff, kicking off my @user talkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14352</th>\n",
       "      <td>0</td>\n",
       "      <td>couple having sex naked arabian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>0</td>\n",
       "      <td>a perfect day to take my fitness outside. wild...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>0</td>\n",
       "      <td>here comes #monsoon2016 #mumbairains ,#delight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24839</th>\n",
       "      <td>0</td>\n",
       "      <td>@user the far left &amp;amp; far right are lappin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22264</th>\n",
       "      <td>1</td>\n",
       "      <td>@user the latest the blicqerâ¢ daily!  thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28718</th>\n",
       "      <td>0</td>\n",
       "      <td>@user   @user should have come out strong agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15167</th>\n",
       "      <td>0</td>\n",
       "      <td>#fathersday to all our #heros !!! let us #ce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "15825      0                               ... such evil acts! \n",
       "7656       0  ditching @user for prom really was #sad   than...\n",
       "28479      0  ready for takeoff, kicking off my @user talkin...\n",
       "14352      0                  couple having sex naked arabian  \n",
       "9897       0  a perfect day to take my fitness outside. wild...\n",
       "1930       0  here comes #monsoon2016 #mumbairains ,#delight...\n",
       "24839      0   @user the far left &amp; far right are lappin...\n",
       "22264      1  @user the latest the blicqerâ¢ daily!  thanks...\n",
       "28718      0  @user   @user should have come out strong agai...\n",
       "15167      0    #fathersday to all our #heros !!! let us #ce..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDHYA_DATA = 'training_data/vidhya_data.csv'\n",
    "NAACL_DATA = 'training_data/NAACL_data.csv'\n",
    "\n",
    "vidhya_data = pd.read_csv(VIDHYA_DATA, usecols=['label', 'tweet'])\n",
    "naacl_data = pd.read_csv(NAACL_DATA, usecols=['label', 'tweet'])\n",
    "\n",
    "training_data = pd.concat([vidhya_data, naacl_data]).reset_index(drop=True)\n",
    "\n",
    "print(training_data.describe())\n",
    "training_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets = list(training_data.tweet)\n",
    "labels = np.array(training_data.label)\n",
    "\n",
    "tweets_tokenized = [t_proc.tokenize_status_text(t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hash for Twitter vocab\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "counter = Counter()\n",
    "for t in tweets_tokenized:\n",
    "    counter.update(t)\n",
    "\n",
    "vocab = [x[0] for x in counter.most_common(VOCAB_SIZE)]\n",
    "\n",
    "# Reserve 0 for unknown tokens\n",
    "hash_dict = dict((vocab[i], i+1) for i in range(VOCAB_SIZE))\n",
    "\n",
    "# Save hashmap for later\n",
    "with open('hashmap.json', 'w', encoding='utf-8') as fh:\n",
    "    json.dump(hash_dict, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndp8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19351c5ac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweet_lens = [len(t) for t in tweets_tokenized]\n",
    "plt.xlim((0,100))\n",
    "plt.hist(tweet_lens, bins=range(0,100,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash and pad tweets\n",
    "MAX_INPUT_LEN = 30 # len 30 should be plenty based on previous histogram\n",
    "\n",
    "def hash_token(t):\n",
    "    hashcode = hash_dict.get(t)\n",
    "    if hashcode is not None:\n",
    "        return hashcode\n",
    "    else:\n",
    "        # Hash unknown tokens as 0\n",
    "        return 0\n",
    "\n",
    "tweets_hashed = [[hash_token(t) for t in tweet] for tweet in tweets_tokenized]\n",
    "tweets_padded = pad_sequences(tweets_hashed, maxlen=MAX_INPUT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding dict from Glove pre-trained embeddings\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_file = 'word_embeddings/glove.twitter.27B.{}d.txt'.format(EMBEDDING_DIM)\n",
    "\n",
    "emb_dict = {}\n",
    "with open(embedding_file, encoding='utf-8') as fh:\n",
    "    for line in fh:\n",
    "        chunks = line.split(' ')\n",
    "        word = chunks[0]\n",
    "        vec = np.array(chunks[1:], dtype='float32')\n",
    "        emb_dict[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix with hashmap and pre-trained embeddings\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM), dtype='float32')\n",
    "for i in range(1, VOCAB_SIZE+1):\n",
    "    embedding_vector = emb_dict.get(vocab[i-1])\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #If the embedding dict doesn't have this token, it will be translated\n",
    "    # as a vector of zeroes.\n",
    "\n",
    "# Build embedding layer to plug into Keras Sequential model\n",
    "embedding = Embedding(input_dim=VOCAB_SIZE+1,\n",
    "                      output_dim=EMBEDDING_DIM,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=MAX_INPUT_LEN,\n",
    "                      trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 100)           5000100   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 5,081,001\n",
      "Trainable params: 80,701\n",
      "Non-trainable params: 5,000,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding)\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(amsgrad=True), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is imbalanced, so we need to figure out what class weights to use\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  np.unique(labels),\n",
    "                                                  labels)\n",
    "\n",
    "#Split into training and validation sets\n",
    "np.random.seed(152)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_padded, labels,\n",
    "                                                    test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43177 samples, validate on 8636 samples\n",
      "Epoch 1/6\n",
      "43177/43177 [==============================] - 23s 522us/step - loss: 0.1481 - acc: 0.9421 - val_loss: 0.1351 - val_acc: 0.9473\n",
      "Epoch 2/6\n",
      "43177/43177 [==============================] - 24s 552us/step - loss: 0.1373 - acc: 0.9456 - val_loss: 0.1205 - val_acc: 0.9538\n",
      "Epoch 3/6\n",
      "43177/43177 [==============================] - 23s 530us/step - loss: 0.1254 - acc: 0.9511 - val_loss: 0.1083 - val_acc: 0.9616\n",
      "Epoch 4/6\n",
      "43177/43177 [==============================] - 24s 545us/step - loss: 0.1144 - acc: 0.9549 - val_loss: 0.0975 - val_acc: 0.9645\n",
      "Epoch 5/6\n",
      "43177/43177 [==============================] - 24s 547us/step - loss: 0.1059 - acc: 0.9588 - val_loss: 0.0888 - val_acc: 0.9676\n",
      "Epoch 6/6\n",
      "43177/43177 [==============================] - 24s 561us/step - loss: 0.0942 - acc: 0.9640 - val_loss: 0.0793 - val_acc: 0.9728\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(tweets_padded, labels,\n",
    "                 batch_size=256, epochs=6,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelv2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
